{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the data set - training data\n",
    "twenty_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'] \n\nI was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n"
    }
   ],
   "source": [
    "# You can check the target names (categories) and some data files by following commands.\n",
    "print(twenty_train.target_names,'\\n') #prints all the categories\n",
    "print(twenty_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class filterData:\n",
    "\n",
    "    def prepareData(self,data):\n",
    "        #Escaping out HTML characters \n",
    "        data=html.unescape(data)\n",
    "    \n",
    "        # remove hyperlinks \n",
    "        data = re.sub(r'https?:\\/\\/.\\S+', \"\", data) \n",
    "        \n",
    "        # remove hashtags\n",
    "        data = re.sub(r'#', '', data) \n",
    "        \n",
    "        # remove old style data text \"RT\" \n",
    "        data = re.sub(r'^RT[\\s]+', '', data)\n",
    "\n",
    "        #dictionary consisting of the contraction and the actual value \n",
    "        dictionary ={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\", \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"} \n",
    "        \n",
    "        #replace the contractions \n",
    "        for key,value in dictionary.items(): \n",
    "            if key in data: \n",
    "                data=data.replace(key,value)\n",
    "\n",
    "        #separate the words \n",
    "        data = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",data) if s]) \n",
    "        \n",
    "        #convert to lower case \n",
    "        data=data.lower()\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def slang(self,data):\n",
    "        #replace the slang word with meaning\n",
    "        file=open(\"slang.txt\",\"r\") \n",
    "        slang=file.read() \n",
    "        \n",
    "        #seperating each line present in the file \n",
    "        slang=slang.split('\\n') \n",
    "        \n",
    "        data_tokens=data.split() \n",
    "        slang_word=[] \n",
    "        meaning=[] \n",
    "        \n",
    "        #store the slang words and meanings in different lists \n",
    "        for line in slang: \n",
    "            temp=line.split(\"=\") \n",
    "            slang_word.append(temp[0]) \n",
    "            meaning.append(temp[-1]) \n",
    "        \n",
    "        #replace the slang word with meaning \n",
    "        for i,word in enumerate(data_tokens): \n",
    "            if word in slang_word: \n",
    "                idx=slang_word.index(word) \n",
    "                data_tokens[i]=meaning[idx] \n",
    "        data=\" \".join(data_tokens)\n",
    "        return data\n",
    "\n",
    "    def stemmAndLemmatization(self,data):\n",
    "        stemmer= PorterStemmer()\n",
    "        lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "        data=word_tokenize(data)\n",
    "        data_list=[] \n",
    "        for word in data:\n",
    "\n",
    "            #word = stemmer.stem(word)\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            data_list.append(word)\n",
    "        data=\" \".join(data_list)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def removeStopWords(self,data):\n",
    "        #import english stopwords list from nltk \n",
    "        stopwords_eng = stopwords.words('english')  \n",
    "        \n",
    "        data_tokens=data.split() \n",
    "        data_list=[] \n",
    "        #remove stopwords \n",
    "        for word in data_tokens: \n",
    "            if word not in stopwords_eng: \n",
    "                data_list.append(word) \n",
    "        data=\" \".join(data_list)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def removepunctuations(self,data):\n",
    "        #remove punctuations\n",
    "        data_tokens=data.split() \n",
    "        data_list=[]  \n",
    "        \n",
    "        for word in data_tokens:\n",
    "            if word not in string.punctuation: \n",
    "                data_list.append(word) \n",
    "        data=\" \".join(data_list)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "data = \"I enjoyed the event which took place , yesteday &amp; I luvd it ! , The link to the show is http://t.co/4ftYom0i It's awsome you'll luv it #HadFun #Enjoyed BFN GN\"\n",
    "\n",
    "data = twenty_train.data[0]\n",
    "print(data,'\\n\\n')\n",
    "\n",
    "filter = filterData()\n",
    "\n",
    "data = filter.prepareData(data)\n",
    "data = filter.slang(data)\n",
    "data = filter.removeStopWords(data)\n",
    "data = filter.stemmAndLemmatization(data)\n",
    "data = filter.removepunctuations(data)\n",
    "\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting features from text files\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#count_vect = CountVectorizer(stop_words='english')\n",
    "#X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "#print(X_train_counts.shape)\n",
    "#print(count_vect.get_feature_names())\n",
    "#print(X_train_counts.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "#X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "#X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning\n",
    "# Training Naive Bayes (NB) classifier on training data.\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "# The names â€˜vectâ€™ , â€˜tfidfâ€™ and â€˜clfâ€™ are arbitrary but will be used later.\n",
    "# We will be using the 'text_clf' going forward.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6062134891131173\n"
    }
   ],
   "source": [
    "# Performance of NB Classifier\n",
    "import numpy as np\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "print(np.mean(predicted == twenty_test.target))\n",
    "#print(text_clf.predict(['i love cars']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6829527349973447"
     },
     "metadata": {},
     "execution_count": 136
    }
   ],
   "source": [
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted_svm == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "# Here, we are creating a list of parameters for which we would like to do performance tuning. \n",
    "# All the parameters name start with the classifier name (remember the arbitrary name we gave). \n",
    "# E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, we create an instance of the grid search by passing the classifier, parameters \n",
    "# and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9157684864695698\n{'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n"
    }
   ],
   "source": [
    "# To see the best mean score and the params, run the following code\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)\n",
    "\n",
    "# Output for above should be: The accuracy has now increased to ~90.6% for the NB classifier (not so naive anymore! ðŸ˜„)\n",
    "# and the corresponding parameters are {â€˜clf__alphaâ€™: 0.01, â€˜tfidf__use_idfâ€™: True, â€˜vect__ngram_rangeâ€™: (1, 2)}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'clf-svm__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# Similarly doing grid search for SVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf-svm__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK\n",
    "# Removing stop words\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8167817312798725"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# Stemming Code\n",
    "\n",
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()), \n",
    "                             ('mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)\n",
    "\n",
    "np.mean(predicted_mnb_stemmed == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. GANDLER)\\nSubject: Need info on 88-89 Bonneville\\nOrganization: University at Buffalo\\nLines: 10\\nNews-Software: VAX/VMS VNEWS 1.41\\nNntp-Posting-Host: ubvmsd.cc.buffalo.edu\\n\\n\\n I am a little confused on all of the models of the 88-89 bonnevilles.\\nI have heard of the LE SE LSE SSE SSEI. Could someone tell me the\\ndifferences are far as features or performance. I am also curious to\\nknow what the book value is for prefereably the 89 model. And how much\\nless than book value can you usually get them for. In other words how\\nmuch are they in demand this time of year. I have heard that the mid-spring\\nearly summer is the best time to buy.\\n\\n\\t\\t\\tNeil Gandler\\n']\nclass is :  sci.med\n"
    }
   ],
   "source": [
    "#print(X_train_tfidf[0])\n",
    "print(twenty_test.data[0:1])\n",
    "#print(text_clf_svm.predict(twenty_test.data[0:1]))\n",
    "print('class is : ',twenty_train.target_names[text_clf_svm.predict(twenty_test.data[5:6])[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1611758599423"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}