{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#NLP\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#Other Technique\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "#CNN\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#Save & Load\n",
    "import pickle\n",
    "import TextNLP\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runNLP(data):\n",
    "    nlpText = TextNLP.TextNLP()\n",
    "    for i , _ in enumerate(data):\n",
    "        data[i] = nlpText.prepareData(data[i])\n",
    "        data[i] = nlpText.slang(data[i])\n",
    "        data[i] = nlpText.removeStopWords(data[i])\n",
    "        data[i] = nlpText.stemmAndLemmatization(data[i])\n",
    "        data[i] = nlpText.removepunctuations(data[i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d4a3501b87cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# ======================= NLP ==========================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunNLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwenty_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunNLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwenty_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-150c4adead9d>\u001b[0m in \u001b[0;36mrunNLP\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlpText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepareData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlpText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslang\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlpText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremoveStopWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlpText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstemmAndLemmatization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\Education\\Git\\Text-Classification-GP\\model\\TextNLP.py\u001b[0m in \u001b[0;36mslang\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mslang\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m#replace the slang word with meaning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"slang.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mslang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[1;34m(do_setlocale)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"win\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf8_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Loading the data set - training data.\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, remove=('headers','footers', 'quotes'))\n",
    "twenty_test  = fetch_20newsgroups(subset='test', shuffle=True, remove=('headers','footers', 'quotes'))\n",
    "# ======================= NLP ========================== \n",
    "train = runNLP(twenty_train.data)\n",
    "test = runNLP(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeY(y, labels):\n",
    "    m = len(y)\n",
    "    vec = np.zeros((m,labels))\n",
    "    for i in range(len(y)):\n",
    "        vec[i][y[i]] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class techniques:\n",
    "\n",
    "    def naiveBayes(self,data, target):\n",
    "        stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "        model = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "        model = model.fit(data, target)\n",
    "        return model\n",
    "\n",
    "    def SVM(self, data, target):\n",
    "        model = Pipeline([('vect',CountVectorizer()), ('tfidf', TfidfTransformer()),('clf-svm', SGDClassifier(loss='hinge',                penalty='l2',alpha=1e-3, max_iter=5, random_state=42))])\n",
    "        model = model.fit(data, target)\n",
    "        return model\n",
    "    \n",
    "    def LinearSVC(self, data, target):\n",
    "        model = Pipeline([('vect',CountVectorizer()), ('tfidf', TfidfTransformer()),('linear-svm', LinearSVC())])\n",
    "        model = model.fit(data, target)\n",
    "        return model\n",
    "\n",
    "    def RandomForest(self, data, target):\n",
    "        model = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf-RF', RandomForestClassifier                       (n_estimators=100, random_state=0, max_depth=20))])\n",
    "        model.fit(data, target)\n",
    "        return model\n",
    "\n",
    "    def CNN(self, data, target, testData):\n",
    "\n",
    "        # Prepare NLP\n",
    "        count_vect = CountVectorizer(stop_words='english')\n",
    "        CV_train = count_vect.fit_transform(data)\n",
    "\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        tfidf_train = tfidf_transformer.fit_transform(CV_train)\n",
    "        \n",
    "        testData = count_vect.transform(testData)\n",
    "        testData = tfidf_transformer.transform(testData)\n",
    "        \n",
    "        data = tfidf_train\n",
    "        target = vectorizeY(target,20)\n",
    "\n",
    "        # Defining the model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(1500, input_dim=data.shape[1], activation='relu'))\n",
    "        model.add(Dense(20, activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Model checkpoints and stoppers. Save the best weights and stops the model when no increase in performance to save time.\n",
    "        esc = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "        cp = ModelCheckpoint(filepath=\"weights.hdf5\", monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "        # fitting the model.\n",
    "        model.fit(data, target, batch_size=128, epochs=4, callbacks=[esc, cp])\n",
    "\n",
    "        return model, testData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= Training =======================\n",
    "technique = techniques()\n",
    "#naiveBayes\n",
    "naiveBayesModel = technique.naiveBayes(train, twenty_train.target)\n",
    "#SVM\n",
    "SVMModel = technique.SVM(train, twenty_train.target)\n",
    "#RandomForest   \n",
    "RandomForestModel = technique.RandomForest(train, twenty_train.target)\n",
    "#LinearSVC\n",
    "LienarSVCModel = technique.LinearSVC(train, twenty_train.target)\n",
    "#CNN\n",
    "CNN, test_tr = technique.CNN(train, twenty_train.target, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================= Testing =======================\n",
    "predicted = LienarSVCModel.predict(test)\n",
    "m = np.mean(predicted == twenty_test.target)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the models\n",
    "\n",
    "#naiveBayesModel\n",
    "with open('savedModels/naiveBayesModel', 'wb') as picklefile:\n",
    "    pickle.dump(naiveBayesModel,picklefile)\n",
    "\n",
    "#SVMModel\n",
    "with open('savedModels/SVMModel', 'wb') as picklefile:\n",
    "    pickle.dump(SVMModel,picklefile)\n",
    "\n",
    "#naiveBayesModel\n",
    "with open('savedModels/RandomForestModel', 'wb') as picklefile:\n",
    "    pickle.dump(RandomForestModel,picklefile)\n",
    "\n",
    "#naiveBayesModel\n",
    "with open('savedModels/LienarSVCModel', 'wb') as picklefile:\n",
    "    pickle.dump(LienarSVCModel,picklefile)\n",
    "\n",
    "#CNN\n",
    "model_json = CNNModel.to_json()\n",
    "with open(\"savedModels/CNN/CNNModel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "#Save weights\n",
    "CNNModel.save_weights(\"savedModels/CNN/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================= Loads Moldes ===================\n",
    "#Way to load first 4 techniques\n",
    "with open('savedModels/naiveBayesModel', 'rb') as training_model:\n",
    "    loaded_model = pickle.load(training_model)\n",
    "\n",
    "#Load the CNN Model\n",
    "#json_file = open('savedModels/CNN/CNNModel.json', 'r')\n",
    "#loaded_model_json = json_file.read()                    #Load model structure from the json file\n",
    "#json_file.close()\n",
    "#loaded_model = model_from_json(loaded_model_json)\n",
    "#loaded_model.load_weights(\"savedModels/CNN/model.h5\")   # load weights into new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text : \n",
      " oh yeah time nice spring/summer day roll window drive around looking bike bike motor opposite direction stick arm hi5'em arm feel like million buck 60km/h thing cyclist problem hi5ing cyclist always right hand lane hafta roll window hi5 back oh well think appreciate thought regard ted \n",
      "\n",
      "Predicted Label :  Motorcycles and related products and laws\n"
     ]
    }
   ],
   "source": [
    "modifiedLabels = ['Discussions of Atheism', 'Computer Graphics','Diverse computer operating system (windows)', \n",
    "                  'Personal computer hardware systems (IBM)', 'Computer systems of Macintosh hardware',\n",
    "                  'Computer windows 10', 'Diverse for-sale', 'Automobiles, automotive products and laws',\n",
    "                  'Motorcycles and related products and laws', 'Discussion about baseball', 'Discussion about hockey.',\n",
    "                  'Cryptography science', 'Electronic Science', 'Medicine science', 'Space Science',\n",
    "                  'religion of society (Christianity)', 'Politics talk about guns', 'Politics talk about middle east',\n",
    "                  'Diverse politics talks', 'Diverse religion talks']\n",
    "\n",
    "def predict(model, text, targets, CNN):\n",
    "    out = model.predict(text)\n",
    "    if(CNN):\n",
    "        out = np.argmax(out,axis=1)\n",
    "    return twenty_train.target_names[out[0]]\n",
    "\n",
    "print('Text :', '\\n',test[50],'\\n')\n",
    "predictedLabel = predict(loaded_model, test_tr[50:51].toarray(), twenty_test.target , True)\n",
    "index = twenty_train.target_names.index(predictedLabel)\n",
    "print('Predicted Label : ' , modifiedLabels[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_tr' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0a03788a85c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#print('Text :', '\\n',test[50],'\\n')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mpredictedLabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m51\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtwenty_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwenty_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictedLabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted Label : '\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmodifiedLabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_tr' is not defined"
     ]
    }
   ],
   "source": [
    "modifiedLabels = ['Discussions of Atheism', 'Computer Graphics','Diverse computer operating system (windows)', \n",
    "                  'Personal computer hardware systems (IBM)', 'Computer systems of Macintosh hardware',\n",
    "                  'Computer windows 10', 'Diverse for-sale', 'Automobiles, automotive products and laws',\n",
    "                  'Motorcycles and related products and laws', 'Discussion about baseball', 'Discussion about hockey.',\n",
    "                  'Cryptography science', 'Electronic Science', 'Medicine science', 'Space Science',\n",
    "                  'religion of society (Christianity)', 'Politics talk about guns', 'Politics talk about middle east',\n",
    "                  'Diverse politics talks', 'Diverse religion talks']\n",
    "\n",
    "def predict(model, text, targets, CNN):\n",
    "    out = model.predict(text)\n",
    "    if(CNN):\n",
    "        out = np.argmax(out,axis=1)\n",
    "    return twenty_train.target_names[out[0]]\n",
    "\n",
    "#print('Text :', '\\n',test[50],'\\n')\n",
    "predictedLabel = predict(loaded_model, test_tr[50:51].toarray(), twenty_test.target , False)\n",
    "index = twenty_train.target_names.index(predictedLabel)\n",
    "print('Predicted Label : ' , modifiedLabels[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "22e98dc126c11ed8b7da0abd0314319f11fb1123801a9053e96dbd9ca057f78a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}