{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#NLP\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#Other Technique\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "#CNN\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#Save & Load\n",
    "import pickle\n",
    "import TextNLP\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runNLP(data):\n",
    "    nlpText = TextNLP.TextNLP()\n",
    "    for i , _ in enumerate(data):\n",
    "        data[i] = nlpText.prepareData(data[i])\n",
    "        data[i] = nlpText.slang(data[i])\n",
    "        data[i] = nlpText.removeStopWords(data[i])\n",
    "        data[i] = nlpText.stemmAndLemmatization(data[i])\n",
    "        data[i] = nlpText.removepunctuations(data[i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading the data set - training data.\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, remove=('headers','footers', 'quotes'))\n",
    "twenty_test  = fetch_20newsgroups(subset='test', shuffle=True, remove=('headers','footers', 'quotes'))\n",
    "# ======================= NLP ========================== \n",
    "train = runNLP(twenty_train.data)\n",
    "test = runNLP(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeY(y, labels):\n",
    "    m = len(y)\n",
    "    vec = np.zeros((m,labels))\n",
    "    for i in range(len(y)):\n",
    "        vec[i][y[i]] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class techniques:\n",
    "\n",
    "    def naiveBayes(self,data, target):\n",
    "        stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "        model = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "        model = model.fit(data, target)\n",
    "        return model\n",
    "\n",
    "    def SVM(self, data, target):\n",
    "        model = Pipeline([('vect',CountVectorizer()), ('tfidf', TfidfTransformer()),('clf-svm', SGDClassifier(loss='hinge',                penalty='l2',alpha=1e-3, max_iter=5, random_state=42))])\n",
    "        model = model.fit(data, target)\n",
    "        return model\n",
    "    \n",
    "    def LinearSVC(self, data, target):\n",
    "        model = Pipeline([('vect',CountVectorizer()), ('tfidf', TfidfTransformer()),('linear-svm', LinearSVC())])\n",
    "        model = model.fit(data, target)\n",
    "        return model\n",
    "\n",
    "    def RandomForest(self, data, target):\n",
    "        model = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf-RF', RandomForestClassifier                       (n_estimators=100, random_state=0, max_depth=20))])\n",
    "        model.fit(data, target)\n",
    "        return model\n",
    "\n",
    "    def CNN(self, data, target, testData):\n",
    "\n",
    "        # Prepare NLP\n",
    "        count_vect = CountVectorizer(stop_words='english')\n",
    "        CV_train = count_vect.fit_transform(data)\n",
    "\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        tfidf_train = tfidf_transformer.fit_transform(CV_train)\n",
    "        \n",
    "        testData = count_vect.transform(testData)\n",
    "        testData = tfidf_transformer.transform(testData)\n",
    "        \n",
    "        data = tfidf_train\n",
    "        target = vectorizeY(target,20)\n",
    "\n",
    "        # Defining the model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(1500, input_dim=data.shape[1], activation='relu'))\n",
    "        model.add(Dense(20, activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Model checkpoints and stoppers. Save the best weights and stops the model when no increase in performance to save time.\n",
    "        esc = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "        cp = ModelCheckpoint(filepath=\"weights.hdf5\", monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "        # fitting the model.\n",
    "        model.fit(data, target, batch_size=128, epochs=4, callbacks=[esc, cp])\n",
    "\n",
    "        return model, testData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= Training =======================\n",
    "technique = techniques()\n",
    "#naiveBayes\n",
    "naiveBayesModel = technique.naiveBayes(train, twenty_train.target)\n",
    "#SVM\n",
    "SVMModel = technique.SVM(train, twenty_train.target)\n",
    "#RandomForest   \n",
    "RandomForestModel = technique.RandomForest(train, twenty_train.target)\n",
    "#LinearSVC\n",
    "LienarSVCModel = technique.LinearSVC(train, twenty_train.target)\n",
    "#CNN\n",
    "CNN, test_tr = technique.CNN(train, twenty_train.target, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================= Testing =======================\n",
    "predicted = LienarSVCModel.predict(test)\n",
    "m = np.mean(predicted == twenty_test.target)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the models\n",
    "\n",
    "#naiveBayesModel\n",
    "with open('savedModels/naiveBayesModel', 'wb') as picklefile:\n",
    "    pickle.dump(naiveBayesModel,picklefile)\n",
    "\n",
    "#SVMModel\n",
    "with open('savedModels/SVMModel', 'wb') as picklefile:\n",
    "    pickle.dump(SVMModel,picklefile)\n",
    "\n",
    "#naiveBayesModel\n",
    "with open('savedModels/RandomForestModel', 'wb') as picklefile:\n",
    "    pickle.dump(RandomForestModel,picklefile)\n",
    "\n",
    "#naiveBayesModel\n",
    "with open('savedModels/LienarSVCModel', 'wb') as picklefile:\n",
    "    pickle.dump(LienarSVCModel,picklefile)\n",
    "\n",
    "#CNN\n",
    "model_json = CNNModel.to_json()\n",
    "with open(\"savedModels/CNN/CNNModel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "#Save weights\n",
    "CNNModel.save_weights(\"savedModels/CNN/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================= Loads Moldes ===================\n",
    "#Way to load first 4 techniques\n",
    "with open('savedModels/LienarSVCModel', 'rb') as training_model:\n",
    "    loaded_model = pickle.load(training_model)\n",
    "\n",
    "#Load the CNN Model\n",
    "json_file = open('savedModels/CNN/CNNModel.json', 'r')\n",
    "loaded_model_json = json_file.read()                    #Load model structure from the json file\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"savedModels/CNN/model.h5\")   # load weights into new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text : \n",
      " oh yeah time nice spring/summer day roll window drive around looking bike bike motor opposite direction stick arm hi5'em arm feel like million buck 60km/h thing cyclist problem hi5ing cyclist always right hand lane hafta roll window hi5 back oh well think appreciate thought regard ted \n",
      "\n",
      "Predicted Label :  Motorcycles and related products and laws\n"
     ]
    }
   ],
   "source": [
    "modifiedLabels = ['Discussions of Atheism', 'Computer Graphics','Diverse computer operating system (windows)', \n",
    "                  'Personal computer hardware systems (IBM)', 'Computer systems of Macintosh hardware',\n",
    "                  'Computer windows 10', 'Diverse for-sale', 'Automobiles, automotive products and laws',\n",
    "                  'Motorcycles and related products and laws', 'Discussion about baseball', 'Discussion about hockey.',\n",
    "                  'Cryptography science', 'Electronic Science', 'Medicine science', 'Space Science',\n",
    "                  'religion of society (Christianity)', 'Politics talk about guns', 'Politics talk about middle east',\n",
    "                  'Diverse politics talks', 'Diverse religion talks']\n",
    "\n",
    "def predict(model, text, targets, CNN):\n",
    "    out = model.predict(text)\n",
    "    if(CNN):\n",
    "        out = np.argmax(out,axis=1)\n",
    "    return twenty_train.target_names[out[0]]\n",
    "\n",
    "print('Text :', '\\n',test[50],'\\n')\n",
    "predictedLabel = predict(loaded_model, test_tr[50:51].toarray(), twenty_test.target , True)\n",
    "index = twenty_train.target_names.index(predictedLabel)\n",
    "print('Predicted Label : ' , modifiedLabels[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.4 64-bit ('machine_learning': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dd611110a353d997d32ab9a3a577087e569cb682f4339466cf5a17f22bab8f94"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}